{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# This notebook will help you find the most common words in any English-language book on Project Gutenberg. \n",
    "\n",
    "Inspired by the DataCamp Project \"Word frequency in Moby Dick\" (https://www.datacamp.com/projects/38), I made this notebook to help a user find the most common words in any book on Project Gutenberg. All you need to do is supply the title of a book and a URL for an HTML file of that book.\n",
    "\n",
    "First we'll read in an HTML file from Project Gutenberg. Then we'll extract just the text and get rid of any uninteresting words (articles, conjunctions, etc.). Finally, we'll count the words to determine the most common one and display the top 25 words on a plot. Ready? Let's get started."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import requests, BeautifulSoup, and nltk\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import nltk\n",
    "\n",
    "# Now run this cell."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Now let's find a book.\n",
    "\n",
    "Navigate to www.gutenberg.org and choose a book. Be sure to choose the HTML version of the book. Copy the url, paste it below, and fill in the book's title where indicated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Type the title of your chosen book between the single quotes.\n",
    "title = ''\n",
    "\n",
    "# Use requests.get to fetch your HTML file. Insert the URL of your chosen book between the single quotes in the line below.\n",
    "request = requests.get('')\n",
    "\n",
    "# Set the encoding of the HTML file to UTF-8.\n",
    "request.encoding = 'utf-8'\n",
    "\n",
    "# Use .text to extract just the text from the HTML file.\n",
    "html = request.text\n",
    "\n",
    "# Check that it is working by printing the first 2000 characters of the text.\n",
    "print(html[0:2000])\n",
    "\n",
    "# Now run this cell."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The next step is to extract the text from this HTML file.\n",
    "\n",
    "We'll use BeautifulSoup to read the HTML file and identify the part that is text. Then .get_text() will help us extract the text of the book from the HTML file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use BeautifulSoup on the HTML file.\n",
    "soup = BeautifulSoup(html, 'html.parser')\n",
    "\n",
    "# Use .get_text() to extract just the text.\n",
    "text = soup.get_text()\n",
    "\n",
    "# Check the results by printing a passage from the middle of the text.\n",
    "print(text[20000:22000])\n",
    "\n",
    "# Now run this cell."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Next, we'll cull the individual words from the text.\n",
    "\n",
    "First, we'll \"tokenize\" the text; this means to throw out things that aren't words, like spaces and punctuation. To do this, we'll use a function from the Natural Language Toolkit (nltk)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a tokenizer function.\n",
    "tokenizer = nltk.tokenize.RegexpTokenizer('\\w+')\n",
    "\n",
    "# Now apply the tokenizer function to the text to extract tokens (words).\n",
    "tokens = tokenizer.tokenize(text)\n",
    "\n",
    "# Check the results by printing the first 10 tokens.\n",
    "print(tokens[0:10])\n",
    "\n",
    "# Now run this cell."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Now we'll make all the tokens lowercase.\n",
    "\n",
    "Why? Well, if a word appears sometimes at the beginning sentence (i.e., capitalized) and sometimes in the middle of a sentence (i.e., uncapitalized), it would get treated like two distinct tokens, when from a human point of view, those two tokens represent the same word. \n",
    "\n",
    "First we'll create an empty list to store our tokens. Then we'll use a \"for loop\" to check each token, make it lowercase as needed, and then store it in the list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make an empty list to store the tokens.\n",
    "words = []\n",
    "\n",
    "# Use a \"for loop\" to make each token lowercase and store it in the list.\n",
    "for word in tokens:\n",
    "    word = word.lower()\n",
    "    words.append(word)\n",
    "\n",
    "# Check the results by printing the first 10 words.\n",
    "print(words[0:10])\n",
    "\n",
    "# Now run this cell."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Let's remove words that are very common but not very interesting (like pronouns).\n",
    "\n",
    "To do this, we'll download a list of \"stopwords\"--very common words like \"the\" and \"and\" that we don't want to consider in our analysis. We'll be using a default list of stopwords from nltk, but it is also possible to create a custom list of stopwords as well.\n",
    "\n",
    "First, we'll download a list of stopwords to use. Then we'll loop over our wordlist and throw out any that appear on the stoplist."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download a list of stopwords from nltk.\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Extract a list of English stopwords to use\n",
    "sw = nltk.corpus.stopwords.words('english')\n",
    "\n",
    "# Make a new empty list for our cleaned-up collection of words.\n",
    "words_clean = []\n",
    "\n",
    "# Get rid of words on the stop list.\n",
    "for word in words:\n",
    "    if word not in sw:\n",
    "        words_clean.append(word)\n",
    "        \n",
    "# Check results by printing the first 10 words on the new list.\n",
    "print(words_clean[0:10])\n",
    "\n",
    "# Now run this cell."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# At last we can see which words are used most frequently in the chosen book.\n",
    "\n",
    "We'll count how often each word occurs and plot the top 25 most common words in a convenient chart."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use matplotlib to set up an inline plot.\n",
    "%matplotlib inline\n",
    "\n",
    "# Calculate word frequencies\n",
    "word_freq = nltk.FreqDist(words_clean)\n",
    "\n",
    "# Plot the 25 most common words\n",
    "word_freq.plot(25)\n",
    "\n",
    "# Now run this cell."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# To summarize, let's print the most common word in the chosen book."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the most common word and print it with the book title.\n",
    "print('The most common word in ' + title + ' is \"' + word_freq.most_common(1)[0][0] + '.\"')\n",
    "print('It occurs ' + str(word_freq.most_common(1)[0][1]) + ' times.')\n",
    "\n",
    "# Now run this cell."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
